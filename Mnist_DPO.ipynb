{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# What Happens When You Train a CNN to Think like an LLM\n",
        "## Understanding DPO with Digit Classification\n",
        "\n",
        "Direct Preference Optimization (DPO) has been the go-to technique for aligning LLMs. It's elegant, it works, and it's everywhere. But most discussions live in the world of transformers, tokenization, and text generation.\n",
        "\n",
        "**The Experiment:**\n",
        "I wanted to strip away all that complexity and see the DPO algorithm in its purest form. So, I trained a **Convolutional Neural Net (CNN)** with DPO to identify handwritten digits (MNIST).\n",
        "\n",
        "**What is DPO Really?**\n",
        "Imagine you're teaching a kid to identify animals. Instead of just saying \"this is correct\" or \"this is wrong,\" you show them pairs: \"This fluffy thing is more likely a dog than a cat\" or \"This one with stripes is definitely a zebra, not a horse.\"\n",
        "\n",
        "That's DPO in a nutshell. Rather than training on absolute labels, you're training on **preferences**. You're telling the model: *\"Between these two options, prefer this one over that one.\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Setup & Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay\n",
        "from tqdm.auto import tqdm\n",
        "import tensorflow as tf # For loading original MNIST easily\n",
        "import os\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. The Data (MNIST)\n",
        "MNIST is the data equivalent of \"Hello World\" in machine learning: 70,000 handwritten digits (0\u20139).\n",
        "\n",
        "The task for a typical MNIST training is simple: *\"Look at an image and tell me which digit it is.\"*\n",
        "\n",
        "**Our DPO training is different:** *\"Whenever you see an image, increase its probability for the preferred choice over the rejected one.\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load standard MNIST using TensorFlow/Keras\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "print(f\"Training data shape: {x_train.shape}\")\n",
        "print(f\"Test data shape: {x_test.shape}\")\n",
        "\n",
        "# Helper function to plot images\n",
        "def plot_mnist_image(index, dataset, title=None):\n",
        "    \"\"\"Plots a single MNIST image from a numpy array or torch tensor.\"\"\"\n",
        "    image = dataset[index]\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image = image.squeeze().cpu().numpy()\n",
        "    elif isinstance(image, np.ndarray) and image.ndim == 3:\n",
        "        image = image.squeeze()\n",
        "    \n",
        "    plt.imshow(image, cmap='gray')\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    else:\n",
        "        plt.title(f\"Index: {index}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Corrupting the Data\n",
        "To make this interesting, we need \"hard\" examples. We will synthetically corrupt the MNIST images by adding Gaussian noise.\n",
        "\n",
        "What was once a crisp \"8\" will now look like it went through a blender."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Synthetic Noisy Data\n",
        "print(\"Generating noisy data...\")\n",
        "\n",
        "# Add Gaussian noise\n",
        "noise_factor = 0.5\n",
        "aug_mnist_noise = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "aug_mnist_noise_test = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "\n",
        "# Clip to valid range [0, 1]\n",
        "aug_mnist_noise = np.clip(aug_mnist_noise, 0., 1.)\n",
        "aug_mnist_noise_test = np.clip(aug_mnist_noise_test, 0., 1.)\n",
        "\n",
        "# Labels remain the same\n",
        "aug_labels = y_train\n",
        "aug_test_labels = y_test\n",
        "\n",
        "print(f\"Augmented Noise Data Shape: {aug_mnist_noise.shape}\")\n",
        "\n",
        "# Visualize a noisy sample\n",
        "plot_mnist_image(0, aug_mnist_noise, title=f\"Noisy Label: {aug_labels[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. The Key Components\n",
        "To apply DPO to any model, you need three key ingredients:\n",
        "\n",
        "1.  **The Policy (Main Model) $\\pi_\\theta$**: This is your student model. It starts off knowing nothing about preferences and learns through the DPO process. In our case, it's a fresh CNN.\n",
        "2.  **The Reference Model $\\pi_{ref}$**: Think of this as your anchor point. It's a frozen, pre-trained model that provides a baseline. The policy model learns to deviate from this reference in the \"right\" direction.\n",
        "3.  **Preferences**: Pairs where one is preferred over the other.\n",
        "    *   $y_w$: For the preferred choice (winner).\n",
        "    *   $y_l$: For the rejected choice (loser).\n",
        "\n",
        "### Step 1: Train the Reference Model ($\\pi_{ref}$)\n",
        "We train a standard CNN on clean MNIST. Nothing fancy \u2014 just a single convolutional layer and a classifier head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PyTorchRefModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PyTorchRefModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(64 * 28 * 28, 10) # 64 channels * 28x28 image size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "ref_model = PyTorchRefModel().to(device)\n",
        "print(\"Reference model initialized.\")\n",
        "\n",
        "# Prepare DataLoaders for Clean Data\n",
        "x_train_tensor = torch.tensor(x_train).unsqueeze(1).float()\n",
        "y_train_tensor = torch.tensor(y_train).long()\n",
        "train_loader = DataLoader(TensorDataset(x_train_tensor, y_train_tensor), batch_size=64, shuffle=True)\n",
        "\n",
        "x_test_tensor = torch.tensor(x_test).unsqueeze(1).float()\n",
        "y_test_tensor = torch.tensor(y_test).long()\n",
        "test_loader = DataLoader(TensorDataset(x_test_tensor, y_test_tensor), batch_size=64, shuffle=False)\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, loader, epochs=2):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1} Loss: {running_loss/len(loader):.4f}\")\n",
        "\n",
        "# Train the Reference Model\n",
        "print(\"Training Reference Model on Clean Data...\")\n",
        "train_model(ref_model, train_loader, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Manufacturing Preferences\n",
        "To make this work without human labeling, we need a strategy to generate preference pairs ($y_w$, $y_l$).\n",
        "\n",
        "**The Strategy:**\n",
        "1.  Feed the **corrupted images** to the Reference Model ($\\pi_{ref}$).\n",
        "2.  Because the images are so noisy, the Reference Model will often fail (confidently wrong).\n",
        "3.  For every corrupted image where $\\pi_{ref}$ fails, we create a pair:\n",
        "    *   **$y_w$ (Winner)**: The Ground Truth label (what the digit really is).\n",
        "    *   **$y_l$ (Loser)**: The Reference Model's incorrect prediction.\n",
        "\n",
        "**The Instruction:** \"On this noisy image, prefer the ground truth ($y_w$) over the reference model's mistake ($y_l$).\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare Noisy Data Loader\n",
        "aug_images = aug_mnist_noise\n",
        "\n",
        "aug_tensor = torch.tensor(aug_images).unsqueeze(1).float()\n",
        "aug_labels_tensor = torch.tensor(aug_labels).long()\n",
        "aug_loader = DataLoader(TensorDataset(aug_tensor, aug_labels_tensor), batch_size=64, shuffle=False)\n",
        "\n",
        "# Run Inference to find mistakes\n",
        "ref_model.eval()\n",
        "all_preds = []\n",
        "print(\"Running inference on noisy data to generate preference pairs...\")\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in aug_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = ref_model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "incorrect_mask = all_preds != aug_labels\n",
        "\n",
        "# Create DPO Training Data\n",
        "dpo_images = aug_tensor[incorrect_mask]\n",
        "dpo_y_plus = aug_labels_tensor[incorrect_mask]       # Chosen (Correct)\n",
        "dpo_y_minus = torch.tensor(all_preds[incorrect_mask]).long() # Rejected (Wrong)\n",
        "\n",
        "print(f\"Total Noisy Images: {len(aug_images)}\")\n",
        "print(f\"Incorrect Predictions (DPO Samples): {len(dpo_images)}\")\n",
        "\n",
        "# Visualize a mistake\n",
        "if len(dpo_images) > 0:\n",
        "    idx = 0\n",
        "    plot_mnist_image(idx, dpo_images, title=f\"True: {dpo_y_plus[idx].item()}, Pred: {dpo_y_minus[idx].item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. DPO Training\n",
        "Now we train the Policy Model using the DPO loss function.\n",
        "\n",
        "### The DPO Formula\n",
        "Don't let the math scare you. Here is what it actually says:\n",
        "\n",
        "$$ \\mathcal{L}_{DPO}(\\pi_\\theta; \\pi_{ref}) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right) \\right] $$\n",
        "\n",
        "**Translation:** \"Make the Policy Model assign a higher probability to the Winner ($y_w$) and a lower probability to the Loser ($y_l$) *relative* to what the Reference Model did.\"\n",
        "\n",
        "The variable $\\beta$ is a **temperature parameter** that controls how tight the leash is. It determines how far the policy model is allowed to wander away from the reference model's behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DPO Dataset Class\n",
        "class DPODataset(Dataset):\n",
        "    def __init__(self, images, y_plus, y_minus):\n",
        "        self.images = images\n",
        "        self.y_plus = y_plus\n",
        "        self.y_minus = y_minus\n",
        "    def __len__(self): return len(self.images)\n",
        "    def __getitem__(self, idx): return self.images[idx], self.y_plus[idx], self.y_minus[idx]\n",
        "\n",
        "dpo_loader = DataLoader(DPODataset(dpo_images, dpo_y_plus, dpo_y_minus), batch_size=64, shuffle=True)\n",
        "\n",
        "# Initialize Policy Model (Clone of Ref Model)\n",
        "policy_model = PyTorchRefModel().to(device)\n",
        "policy_model.load_state_dict(ref_model.state_dict())\n",
        "\n",
        "# DPO Loss Function\n",
        "def dpo_loss(policy_logits, ref_logits, y_chosen, y_rejected, beta=0.1):\n",
        "    # Convert logits to log probabilities\n",
        "    policy_log_probs = F.log_softmax(policy_logits, dim=1)\n",
        "    ref_log_probs = F.log_softmax(ref_logits, dim=1)\n",
        "    \n",
        "    # Gather log probs for chosen and rejected classes\n",
        "    policy_chosen_log_prob = policy_log_probs.gather(1, y_chosen.unsqueeze(1)).squeeze()\n",
        "    policy_rejected_log_prob = policy_log_probs.gather(1, y_rejected.unsqueeze(1)).squeeze()\n",
        "    \n",
        "    ref_chosen_log_prob = ref_log_probs.gather(1, y_chosen.unsqueeze(1)).squeeze()\n",
        "    ref_rejected_log_prob = ref_log_probs.gather(1, y_rejected.unsqueeze(1)).squeeze()\n",
        "    \n",
        "    # Calculate logits for the sigmoid\n",
        "    # (log(pi(yw)/ref(yw)) - log(pi(yl)/ref(yl)))\n",
        "    logits = beta * ((policy_chosen_log_prob - ref_chosen_log_prob) - (policy_rejected_log_prob - ref_rejected_log_prob))\n",
        "    \n",
        "    # Loss is negative log sigmoid\n",
        "    loss = -F.logsigmoid(logits).mean()\n",
        "    return loss\n",
        "\n",
        "# DPO Training Loop\n",
        "optimizer = optim.Adam(policy_model.parameters(), lr=0.001)\n",
        "print(\"Starting DPO Training...\")\n",
        "\n",
        "ref_model.eval() # Reference model must be frozen/eval mode\n",
        "policy_model.train()\n",
        "\n",
        "for epoch in range(5):\n",
        "    running_loss = 0.0\n",
        "    for inputs, y_c, y_r in tqdm(dpo_loader, desc=f\"DPO Epoch {epoch+1}\"):\n",
        "        inputs, y_c, y_r = inputs.to(device), y_c.to(device), y_r.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        policy_logits = policy_model(inputs)\n",
        "        \n",
        "        # Get ref logits (no grad)\n",
        "        with torch.no_grad():\n",
        "            ref_logits = ref_model(inputs)\n",
        "            \n",
        "        loss = dpo_loss(policy_logits, ref_logits, y_c, y_r)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} DPO Loss: {running_loss/len(dpo_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Results and Key Insights\n",
        "We compare the accuracy of the original Reference Model and the new Policy Model on the noisy test set.\n",
        "\n",
        "**Why does this matter?**\n",
        "You might ask: *\"Why not just train a supervised model on the noisy data?\"*\n",
        "\n",
        "Here is the critical distinction:\n",
        "*   **Supervised Learning** asks: *\"What is the truth?\"* It tries to maximize the probability of the correct digit (e.g., 7).\n",
        "*   **DPO** asks: *\"What is preferred?\"* It explicitly tries to widen the gap between the correct digit (7) and the *specific error* the reference model makes (e.g., 1).\n",
        "\n",
        "In objective tasks like digit classification, \"truth\" is all you need. But in subjective tasks (like LLM responses), \"truth\" is fuzzy, and \"preference\" becomes the most powerful signal we have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, loader, name=\"Model\"):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    acc = correct / total\n",
        "    print(f\"{name} Accuracy on Noisy Test Data: {acc:.4f}\")\n",
        "\n",
        "# Prepare Noisy Test Loader\n",
        "aug_test_images = aug_mnist_noise_test\n",
        "aug_test_tensor = torch.tensor(aug_test_images).unsqueeze(1).float()\n",
        "aug_test_labels_tensor = torch.tensor(aug_test_labels).long()\n",
        "aug_test_loader = DataLoader(TensorDataset(aug_test_tensor, aug_test_labels_tensor), batch_size=64, shuffle=False)\n",
        "\n",
        "# Compare\n",
        "print(\"--- Results ---\")\n",
        "evaluate(ref_model, aug_test_loader, \"Reference Model (Baseline)\")\n",
        "evaluate(policy_model, aug_test_loader, \"Policy Model (DPO Trained)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Words\n",
        "This experiment wasn't really about handwritten digits. It was about demystifying the algorithm that currently aligns the world's most powerful models.\n",
        "\n",
        "We proved that **DPO isn't some magic that only lives inside a Transformer**. It is a model-agnostic mathematical framework.\n",
        "*   In our case, we used it to help a CNN navigate **visual noise** (blur).\n",
        "*   In the real world, researchers use the exact same math to help LLMs navigate **semantic noise** (hallucinations, toxicity, and style errors).\n",
        "\n",
        "By stripping away the complexity of tokenizers and billions of parameters, we can see DPO for what it truly is: **A way to teach a model not just what is right, but specifically what is wrong.**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}